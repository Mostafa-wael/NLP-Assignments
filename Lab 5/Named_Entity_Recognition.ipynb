{"cells":[{"cell_type":"code","execution_count":23,"metadata":{"id":"PXjGQSypydnl"},"outputs":[],"source":["# Type Your full names\n","Student_1 = \"Mostafa Mohamed Elgendy\"\n","Student_2 = \"Mostafa Wael Kamal\""]},{"cell_type":"markdown","metadata":{"id":"hVqrsVWh0kiC"},"source":["# Named Entity Recognition Assignment\n","NER is a subtask of information extraction that locates and classifies named entities in a text. The named entities could be organizations, persons, locations, times, etc. In this assignment, you will train a named entity recognition system and test it on a test data. \\\n","Let's get started"]},{"cell_type":"code","execution_count":24,"metadata":{"id":"WR6a6DkN0d-3"},"outputs":[],"source":["import os \n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm\n","import torch\n","from torch import nn\n","from utils import get_params, get_vocab\n","import random as rnd"]},{"cell_type":"markdown","metadata":{"id":"_44BK5K82YwF"},"source":["# Importing and discovering the data"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"ulSik2Sv1p1G"},"outputs":[],"source":["vocab, tag_map = get_vocab('data/large/words.txt', 'data/large/tags.txt')\n","t_sentences, t_labels, t_size = get_params(vocab, tag_map, 'data/large/train/sentences.txt', 'data/large/train/labels.txt')\n","v_sentences, v_labels, v_size = get_params(vocab, tag_map, 'data/large/val/sentences.txt', 'data/large/val/labels.txt')\n","test_sentences, test_labels, test_size = get_params(vocab, tag_map, 'data/large/test/sentences.txt', 'data/large/test/labels.txt')"]},{"cell_type":"markdown","metadata":{"id":"Y-PkKTK22xc6"},"source":["`vocab` is a dictionary that translates a word string to a unique number. Given a sentence, you can represent it as an array of numbers translating with this dictionary. The dictionary contains a `<PAD>` token. \n","\n","When training an LSTM using batches, all your input sentences must be the same size. To accomplish this, you set the length of your sentences to a certain number and add the generic `<PAD>` token to fill all the empty spaces. "]},{"cell_type":"code","execution_count":26,"metadata":{"id":"IB_1MhtP1rLL"},"outputs":[{"name":"stdout","output_type":"stream","text":["vocab[\"the\"]: 9\n","padded token: 35180\n"]}],"source":["# vocab translates from a word to a unique number\n","print('vocab[\"the\"]:', vocab[\"the\"])\n","# Pad token\n","print('padded token:', vocab['<PAD>'])"]},{"cell_type":"code","execution_count":27,"metadata":{"id":"PagjN4rl22Fr"},"outputs":[{"name":"stdout","output_type":"stream","text":["{'O': 0, 'B-geo': 1, 'B-gpe': 2, 'B-per': 3, 'I-geo': 4, 'B-org': 5, 'I-org': 6, 'B-tim': 7, 'B-art': 8, 'I-art': 9, 'I-per': 10, 'I-gpe': 11, 'I-tim': 12, 'B-nat': 13, 'B-eve': 14, 'I-eve': 15, 'I-nat': 16}\n"]}],"source":["# The possible tags\n","print(tag_map)"]},{"cell_type":"markdown","metadata":{"id":"Y6e9sGin3FQ-"},"source":["So the coding scheme that tags the entities is a minimal one where B- indicates the first token in a multi-token entity, and I- indicates one in the middle of a multi-token entity. If you had the sentence \n","\n","**\"Sharon flew to Miami on Friday\"**\n","\n","the outputs would look like:\n","\n","```\n","Sharon B-per\n","flew   O\n","to     O\n","Miami  B-geo\n","on     O\n","Friday B-tim\n","```\n","\n","your tags would reflect three tokens beginning with B-, since there are no multi-token entities in the sequence. But if you added Sharon's last name to the sentence: \n","\n","**\"Sharon Floyd flew to Miami on Friday\"**\n","\n","```\n","Sharon B-per\n","Floyd  I-per\n","flew   O\n","to     O\n","Miami  B-geo\n","on     O\n","Friday B-tim\n","```\n","\n","then your tags would change to show first \"Sharon\" as B-per, and \"Floyd\" as I-per, where I- indicates an inner token in a multi-token sequence."]},{"cell_type":"code","execution_count":28,"metadata":{"id":"oWLR2Oxp28K6"},"outputs":[{"name":"stdout","output_type":"stream","text":["The number of outputs is tag_map 17\n","Num of vocabulary words: 35181\n","The vocab size is 35181\n","The training size is 33570\n","The validation size is 7194\n","An example of the first sentence is [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 9, 15, 1, 16, 17, 18, 19, 20, 21]\n","An example of its corresponding label is [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0]\n"]}],"source":["# Exploring information about the data\n","print('The number of outputs is tag_map', len(tag_map))\n","# The number of vocabulary tokens (including <PAD>)\n","g_vocab_size = len(vocab)\n","print(f\"Num of vocabulary words: {g_vocab_size}\")\n","print('The vocab size is', len(vocab))\n","print('The training size is', t_size)\n","print('The validation size is', v_size)\n","print('An example of the first sentence is', t_sentences[0])\n","print('An example of its corresponding label is', t_labels[0])"]},{"cell_type":"markdown","metadata":{"id":"wt3e4nxjFT3O"},"source":["# NERDataset\n","The class that impelements the dataset for NER"]},{"cell_type":"code","execution_count":29,"metadata":{"id":"29iM0u4-4YOV"},"outputs":[],"source":["class NERDataset(torch.utils.data.Dataset):\n","\n","  def __init__(self, x, y, pad):\n","    \"\"\"\n","    This is the constructor of the NERDataset\n","    Inputs:\n","    - x: a list of lists where each list contains the ids of the tokens\n","    - y: a list of lists where each list contains the label of each token in the sentence\n","    - pad: the id of the <PAD> token (to be used for padding all sentences and labels to have the same length)\n","    \"\"\"\n","    # pad every column in x and y to have the same length\n","    # create two tensors one for x and the other for labels\n","    # self.x = torch.tensor([sentence + [pad] * (max([len(sentence) for sentence in x]) - len(sentence)) for sentence in x])\n","    # self.y = torch.tensor([sentence + [0] * (max([len(sentence) for sentence in y]) - len(sentence)) for sentence in y])\n","\n","    self.x = nn.utils.rnn.pad_sequence(\n","        [torch.tensor(i) for i in x], batch_first=True, padding_value=pad)\n","    self.y = nn.utils.rnn.pad_sequence(\n","        [torch.tensor(j) for j in y], batch_first=True, padding_value=0)\n","    \n","    #################################################################################################################\n","\n","  def __len__(self):\n","    \"\"\"\n","    This function should return the length of the dataset (the number of sentences)\n","    \"\"\"\n","    # return the length of the dataset\n","    return len(self.x)\n","    ###########################################################################################\n","\n","  def __getitem__(self, idx):\n","    \"\"\"\n","    This function returns a subset of the whole dataset\n","    \"\"\"\n","    # return a tuple of x and y\n","    return self.x[idx], self.y[idx]\n","    ##########################################################################################"]},{"cell_type":"code","execution_count":30,"metadata":{"id":"sz-saCtRs7Pz"},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([5, 30]) torch.Size([5, 30]) torch.Size([3, 30]) torch.Size([3, 30])\n","tensor([    0,     1,     2,     3,     4,     5,     6,     7,     8,     9,\n","           10,    11,    12,    13,    14,     9,    15,     1,    16,    17,\n","           18,    19,    20,    21, 35180, 35180, 35180, 35180, 35180, 35180]) \n"," tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0])\n"]}],"source":["batch_size = 5\n","mini_sentences = t_sentences[0: 8]\n","mini_labels = t_labels[0: 8]\n","mini_dataset = NERDataset(mini_sentences, mini_labels, vocab['<PAD>'])\n","dummy_dataloader = torch.utils.data.DataLoader(mini_dataset, batch_size=5)\n","dg = iter(dummy_dataloader)\n","X1, Y1 = next(dg)\n","X2, Y2 = next(dg)\n","print(Y1.shape, X1.shape, Y2.shape, X2.shape)\n","print(X1[0][:], \"\\n\", Y1[0][:])"]},{"cell_type":"markdown","metadata":{"id":"Jfw9pEd1wdMr"},"source":["#### Expected output\n","torch.Size([5, 30]) torch.Size([5, 30]) torch.Size([3, 30]) torch.Size([3, 30])\\\n","tensor([    0,     1,     2,     3,     4,     5,     6,     7,     8,     9,\n","           10,    11,    12,    13,    14,     9,    15,     1,    16,    17,\n","           18,    19,    20,    21, 35180, 35180, 35180, 35180, 35180, 35180]) \\\n","tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0])"]},{"cell_type":"markdown","metadata":{"id":"CQB6O7I7FbUh"},"source":["# NER\n","The class that implementss the pytorch model for NER"]},{"cell_type":"code","execution_count":31,"metadata":{"id":"xHeJcz1JuhYa"},"outputs":[],"source":["class NER(nn.Module):\n","  def __init__(self, vocab_size=35181, embedding_dim=50, hidden_size=50, n_classes=len(tag_map)):\n","    \"\"\"\n","    The constructor of our NER model\n","    Inputs:\n","    - vacab_size: the number of unique words\n","    - embedding_dim: the embedding dimension\n","    - n_classes: the number of final classes (tags)\n","    \"\"\"\n","    super(NER, self).__init__()\n","    # (1) Create the embedding layer\n","    self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=vocab['<PAD>'])\n","\n","    # (2) Create an LSTM layer with hidden size = hidden_size and batch_first = True\n","    self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_size, batch_first=True)\n","\n","    # (3) Create a linear layer with number of neorons = n_classes\n","    self.linear = nn.Linear(hidden_size, n_classes)\n","    #####################################################################################################\n","\n","  def forward(self, sentences):\n","    \"\"\"\n","    This function does the forward pass of our model\n","    Inputs:\n","    - sentences: tensor of shape (batch_size, max_length)\n","\n","    Returns:\n","    - final_output: tensor of shape (batch_size, max_length, n_classes)\n","    \"\"\"\n","    # (1) Pass the sentences through the embedding layer\n","    embeddingsOutput = self.embedding(sentences)\n","\n","    # (2) Pass the output of the embedding layer through the LSTM layer\n","    lstmOutput, _ = self.lstm(embeddingsOutput)\n","\n","    # (3) Pass the output of the LSTM layer through the linear layer\n","    final_output = self.linear(lstmOutput)\n","    \n","    ###############################################################################################\n","    return final_output"]},{"cell_type":"code","execution_count":32,"metadata":{"id":"lJJJF-qQA_wk"},"outputs":[{"name":"stdout","output_type":"stream","text":["NER(\n","  (embedding): Embedding(35181, 50, padding_idx=35180)\n","  (lstm): LSTM(50, 50, batch_first=True)\n","  (linear): Linear(in_features=50, out_features=17, bias=True)\n",")\n"]}],"source":["model = NER()\n","print(model)"]},{"cell_type":"markdown","metadata":{"id":"RCI_6UCRBk6N"},"source":["#### Expected output\n","NER( \\\n","  (embedding): Embedding(35181, 50) \\\n","  (lstm): LSTM(50, 50, batch_first=True) \\\n","  (linear): Linear(in_features=50, out_features=17, bias=True) \\\n",")"]},{"cell_type":"markdown","metadata":{"id":"PLHx_oHpFlSX"},"source":["# Training"]},{"cell_type":"code","execution_count":33,"metadata":{"id":"-yvaq8i2CCLD"},"outputs":[],"source":["def train(model, train_dataset, batch_size=512, epochs=5, learning_rate=0.01):\n","  \"\"\"\n","  This function implements the training logic\n","  Inputs:\n","  - model: the model ot be trained\n","  - train_dataset: the training set of type NERDataset\n","  - batch_size: integer represents the number of examples per step\n","  - epochs: integer represents the total number of epochs (full training pass)\n","  - learning_rate: the learning rate to be used by the optimizer\n","  \"\"\"\n","  \n","  # (1) create the dataloader of the training set (make the shuffle=True)\n","  train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","\n","  # (2) make the criterion cross entropy loss\n","  criterion = torch.nn.CrossEntropyLoss()\n","\n","  # (3) create the optimizer (Adam)\n","  optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","  # GPU configuration\n","  use_cuda = torch.cuda.is_available()\n","  device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","  if use_cuda:\n","    model = model.cuda()\n","    criterion = criterion.cuda()\n","  print(\"Using device: \", device)\n","  for epoch_num in range(epochs):\n","    total_acc_train = 0\n","    total_loss_train = 0\n","    for train_input, train_label in tqdm(train_dataloader):\n","\n","      # (4) move the train input to the device\n","      train_input = train_input.to(device)\n","\n","      # (5) move the train label to the device\n","      train_label = train_label.to(device)\n","\n","      # (6) do the forward pass\n","      output = model(train_input)\n","      \n","      # (7) loss calculation (you need to think in this part how to calculate the loss correctly)\n","      batch_loss = criterion(output.view(-1, output.shape[-1]), train_label.view(-1))\n","\n","      # (8) append the batch loss to the total_loss_train\n","      total_loss_train += batch_loss.item()\n","      \n","      # (9) calculate the batch accuracy (just add the number of correct predictions)\n","      # acc = (torch.argmax(output, dim=-1) == train_label).sum().item()\n","      # or we can calculate the accuracy without using the argmax function\n","      acc = (output.argmax(dim=-1) == train_label).sum().item()\n","      total_acc_train += acc\n","\n","      # (10) zero your gradients\n","      optimizer.zero_grad()\n","\n","      # (11) do the backward pass\n","      batch_loss.backward()\n","\n","      # (12) update the weights with your optimizer\n","      optimizer.step()\n","      \n","    # epoch loss\n","    epoch_loss = total_loss_train / len(train_dataset)\n","\n","    # (13) calculate the accuracy\n","    epoch_acc = total_acc_train / (len(train_dataset) * train_dataset[0][0].shape[0])\n","\n","    print(\n","        f'Epochs: {epoch_num + 1} | Train Loss: {epoch_loss} \\\n","        | Train Accuracy: {epoch_acc}\\n')\n","\n","  ##############################################################################################################"]},{"cell_type":"code","execution_count":34,"metadata":{"id":"3BI7_ANkLf7G"},"outputs":[],"source":["train_dataset = NERDataset(t_sentences, t_labels, vocab['<PAD>'])\n","val_dataset = NERDataset(v_sentences, v_labels, vocab['<PAD>'])\n","test_dataset = NERDataset(test_sentences, test_labels, vocab['<PAD>'])"]},{"cell_type":"code","execution_count":35,"metadata":{"id":"LMXjDv51LU6k"},"outputs":[{"name":"stdout","output_type":"stream","text":["Using device:  cpu\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 132/132 [00:17<00:00,  7.60it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epochs: 1 | Train Loss: 0.0009047294219210632         | Train Accuracy: 0.9587028253248092\n","\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 132/132 [00:18<00:00,  7.00it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epochs: 2 | Train Loss: 0.0001874915819358783         | Train Accuracy: 0.9876833138562361\n","\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 132/132 [00:19<00:00,  6.67it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epochs: 3 | Train Loss: 0.00012044604686814332         | Train Accuracy: 0.9913518824041613\n","\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 132/132 [00:17<00:00,  7.45it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epochs: 4 | Train Loss: 9.623553774782614e-05         | Train Accuracy: 0.9927582433949727\n","\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 132/132 [00:21<00:00,  6.28it/s]"]},{"name":"stdout","output_type":"stream","text":["Epochs: 5 | Train Loss: 8.290905526503233e-05         | Train Accuracy: 0.9935834421759355\n","\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["train(model, train_dataset, batch_size=256, epochs=5)"]},{"cell_type":"markdown","metadata":{"id":"Ko-pv4AOYDMT"},"source":["#### Expected train accuracy after 5 epochs to be above 0.99"]},{"cell_type":"markdown","metadata":{"id":"TWJNO6mUXPRI"},"source":["# Evaluation"]},{"cell_type":"code","execution_count":36,"metadata":{"id":"Gz5mxUAJM1xS"},"outputs":[],"source":["def evaluate(model, test_dataset, batch_size=256):\n","  \"\"\"\n","  This function takes a NER model and evaluates its performance (accuracy) on a test data\n","  Inputs:\n","  - model: a NER model\n","  - test_dataset: dataset of type NERDataset\n","  \"\"\"\n","  # (1) create the test data loader\n","  test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n","\n","  # GPU Configuration\n","  use_cuda = torch.cuda.is_available()\n","  device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","  if use_cuda:\n","    model = model.cuda()\n","\n","  total_acc_test = 0\n","  \n","  # (2) disable gradients\n","  with torch.no_grad():\n","\n","    for test_input, test_label in tqdm(test_dataloader):\n","      # (3) move the test input to the device\n","      test_label = test_label.to(device)\n","\n","      # (4) move the test label to the device\n","      test_input = test_input.to(device)\n","\n","      # (5) do the forward pass\n","      output = model(test_input)\n","\n","      # accuracy calculation (just add the correct predicted items to total_acc_test)\n","      acc = (torch.argmax(output, dim=-1) == test_label).sum().item()\n","      total_acc_test += acc\n","    \n","    # (6) calculate the over all accuracy\n","    total_acc_test /= (len(test_dataset) * test_dataset[0][0].shape[0])\n","  ##################################################################################################\n","\n","  \n","  print(f'\\nTest Accuracy: {total_acc_test}')"]},{"cell_type":"code","execution_count":37,"metadata":{"id":"6FD8JNcHWmMY"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 29/29 [00:01<00:00, 23.89it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Test Accuracy: 0.9860101672028277\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["evaluate(model, test_dataset)"]},{"cell_type":"markdown","metadata":{"id":"zYC6H_6PYDMU"},"source":["#### Expected test accuracy to be above 0.98"]},{"cell_type":"markdown","metadata":{"id":"OhbNZ1HVaLO_"},"source":["# Thank you"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6"},"vscode":{"interpreter":{"hash":"916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"}}},"nbformat":4,"nbformat_minor":0}
